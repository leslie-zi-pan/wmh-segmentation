{"cells":[{"cell_type":"markdown","metadata":{"id":"oLZYyr5H_pNi"},"source":["# Image Slice inferencing\n","Testing image slice inferenceing for microservice setup"]},{"cell_type":"markdown","metadata":{"id":"6QJ6aGIm_vA0"},"source":["# 1) Imports and mount"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","# This sets up the appropriate logging and path configs\n","from notebook_setup import * "]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":5982,"status":"ok","timestamp":1619277912972,"user":{"displayName":"Breadboy Kid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIQJbgS8wbsEiEbFPVlrhQKwF9c9Wja8OVunfx=s64","userId":"05122309557720442969"},"user_tz":-60},"id":"5CDMFoxe_8UN"},"outputs":[],"source":["import torch\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","\n","from monai.losses import DiceLoss\n","from src.features.build_features import train_transform, val_transform, test_transform\n","\n","from src.visualization.visualize import view_slice\n","\n","from monai.data import Dataset, DataLoader\n","from src.enums import DataDict\n","\n","from src.settings.config import get_app_settings\n","from src.utils import load_yaml"]},{"cell_type":"markdown","metadata":{},"source":["## 2) Import Settings"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["settings = get_app_settings()\n","model_config = load_yaml(\"model_configs.yaml\")"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["from src.enums import INTERIM_TESTING_DATA_PATHS, INTERIM_TRAINING_DATA_PATHS\n","from src.pytorch_utils import get_interim_data_path\n","\n","\n","train_paths = get_interim_data_path(INTERIM_TRAINING_DATA_PATHS)\n","val_paths = get_interim_data_path(INTERIM_TESTING_DATA_PATHS)\n","\n","train_dataset  = Dataset(train_paths, train_transform)\n","validation_dataset  = Dataset(val_paths, val_transform)\n","test_dataset = Dataset(val_paths, test_transform)\n","\n","# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=2)\n","# validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE_VAL, shuffle=True)\n","\n","# from monai.transforms import (LoadImaged)\n","# image_loader = LoadImaged(keys=[DataDict.ImageT1, DataDict.ImageFlair])\n","# image_loader(val_paths)\n","# [item for item in dir(nib.load(train_paths[0][DataDict.ImageFlair])) if not item.startswith('_')]\n","test_dataset.__getitem__(0)[DataDict.ImageFlair].shape"]},{"cell_type":"markdown","metadata":{"id":"PzTP28gdGW9J"},"source":["## 3) Inference\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1) Infernce on volume nii"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["'data/raw/test/Singapore/70/wmh.nii.gz'"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["# Get sample volume\n","from src.data.make_dataset import get_test_raw_paths\n","from src.enums import INTERIM_TESTING_DATA_PATHS\n","from src.pytorch_utils import get_interim_data_path\n","from copy import copy\n","import nibabel as nib\n","from src.pytorch_utils import slice_tensor_volume\n","\n","raw_test_paths = get_test_raw_paths()\n","sample_inference_images = copy(raw_test_paths[0])\n","\n","# Load up the nibabel file, not as a path to mimick serving\n","t1_sample = nib.load(sample_inference_images[DataDict.ImageT1]).get_fdata()\n","flair_sample = nib.load(sample_inference_images[DataDict.ImageFlair]).get_fdata()\n","\n","sample_inference_images[DataDict.ImageT1] = t1_sample\n","sample_inference_images[DataDict.ImageFlair] = flair_sample\n","sample_inference_images.pop(DataDict.Label)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["'data/raw/test/Singapore/70/wmh.nii.gz'"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","# sample = copy(raw_test_paths[0])\n","# import nibabel as nib\n","# import numpy as np\n","\n","# t1_sample = nib.load(sample[DataDict.ImageT1]).get_fdata()\n","# flair_sample = nib.load(sample[DataDict.ImageFlair]).get_fdata()\n","\n","\n","# t1_sample = torch.Tensor([nib.Nifti1Image(t1_sample, affine=np.eye(4)).get_fdata()])\n","# flair_sample = torch.Tensor(\n","#     [nib.Nifti1Image(flair_sample, affine=np.eye(4)).get_fdata()]\n","# )\n","\n","\n","# # Set it in the sample\n","# sample[DataDict.ImageT1] = t1_sample\n","# sample[DataDict.ImageFlair] = flair_sample\n","# sample.pop(DataDict.Label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Actual Depency Code!!!!\n","\n","def predict():"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(232, 256, 48)\n"]}],"source":["from src.pytorch_utils import normalize_img_intensity_range\n","\n","\n","sample_data_paths = [sample_inference_images]\n","\n","def slice_inference_data(vol):\n","    slices = slice_tensor_volume(vol)\n","    return slices\n","\n","print(t1_sample.shape)\n","\n","t1_sample = normalize_img_intensity_range(t1_sample)\n","flair_sample = normalize_img_intensity_range(flair_sample)\n","\n","t1_slices = slice_inference_data(t1_sample)\n","flair_slices = slice_inference_data(flair_sample)\n","\n","inference_dict = [{\n","    DataDict.Id: -1,\n","    DataDict.ImageT1: v,\n","    DataDict.ImageFlair: flair_slices.get(idx),\n","    DataDict.DepthZ: idx,\n","} for idx, v in t1_slices.items()]"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 232, 256, 48])"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["from monai.transforms import (\n","    Compose,\n","    ToTensord,\n","    LoadImaged,\n","    EnsureChannelFirstd,\n","    ToMetaTensord,\n","    Spacingd,\n","    Resized,\n","    Orientationd,\n",")\n","from src.features.transforms import ImagesToMultiChannel\n","\n","custom_transform = Compose(\n","    [\n","        # ToMetaTensord(keys=[DataDict.ImageT1, DataDict.ImageFlair]),\n","        EnsureChannelFirstd(\n","            keys=[DataDict.ImageT1, DataDict.ImageFlair], \n","            channel_dim=\"no_channel\"\n","        ),\n","        # Spacingd(\n","        #     keys=[DataDict.ImageT1, DataDict.ImageFlair],\n","        #     # pixdim=(1.5, 1.5),\n","        #     # mode=(\"bilinear\", \"bilinear\"),\n","        #     pixdim=(1.5, 1.5, 2.0),\n","        #     mode=(\"bilinear\", \"bilinear\", \"nearest\"),\n","        # ),\n","        # Resized(\n","        #     keys=[DataDict.ImageT1, DataDict.ImageFlair],\n","        #     spatial_size=[256, 256],\n","        # ),\n","        # Orientationd(keys=[DataDict.ImageT1, DataDict.ImageFlair], axcodes=\"RAS\"),\n","        # ToTensord(keys=[DataDict.ImageT1, DataDict.ImageFlair]),\n","        # ImagesToMultiChannel(keys=[DataDict.ImageT1, DataDict.ImageFlair]),\n","    ]\n",")\n","\n","test_dataset = Dataset(sample_data_paths, custom_transform)\n","test_dataset.__getitem__(0)[DataDict.ImageFlair].shape\n","# view_slice(test_dataset.__getitem__(0)[DataDict.ImageT1][..., 30])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["[<DataDict.Id: 'subj_id'>,\n"," <DataDict.Image: 'image'>,\n"," <DataDict.ImageFlair: 'img_flair'>,\n"," <DataDict.ImageT1: 'img_t1'>,\n"," <DataDict.DepthZ: 'depth_z'>]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["list(test_dataset.__getitem__(0).keys())"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicting 1/1 slices\n","1 subjects to predict\n","Reconstructing 1/1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\zifen\\Documents\\Projects\\wmh-segmentation\\venv\\Lib\\site-packages\\monai\\transforms\\spatial\\array.py:635: UserWarning: axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n","Orientation: spatial shape = (256, 256), channels = 1,please make sure the input is in the channel-first format.\n","  warnings.warn(\n","c:\\Users\\zifen\\Documents\\Projects\\wmh-segmentation\\venv\\Lib\\site-packages\\torch\\_tensor.py:1386: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  ret = func(*args, **kwargs)\n"]},{"data":{"text/plain":["torch.Size([256, 256, 1])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from src.models.predict_model import ImagePredictor\n","from src.models.train_model import model\n","\n","pred_network = model\n","checkpoint = torch.load(\n","    \"models/single_slice_t1_flair_v1.pt\", map_location=torch.device(\"cpu\")\n",")\n","pred_network.load_state_dict(checkpoint[\"model_state_dict\"])\n","img_predictor = ImagePredictor(pred_network, test_dataset)\n","test_predictions = img_predictor.predict_handler()\n","# print('DONE')\n","test_predictions[0][DataDict.Prediction].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from monai.inferers import inferer\n","from monai.transforms import Compose\n","\n","model.eval()\n","device = torch.device(\"cpu\")\n","transform = Compose([ToTensor(), LoadImage(image_only=True)])\n","data = transform(sample_data_paths).to(device)\n","with torch.no_grad():\n","    pred = inferer(inputs=data, network=model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN7ODof/Awz06Myl5HRJxXN","collapsed_sections":[],"machine_shape":"hm","name":"Brain_Segmentation_2D_UNET.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"bf59727a64a2cdf28e31ff587c7cd2fee9be1e9a4e9d3728f183cf44bb7f06e6"}}},"nbformat":4,"nbformat_minor":0}
